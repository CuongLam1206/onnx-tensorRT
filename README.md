# YOLOv8 vá»›i ONNX & TensorRT

Dá»± Ã¡n há»c táº­p vá» ONNX vÃ  TensorRT thÃ´ng qua viá»‡c triá»ƒn khai YOLOv8 object detection.

## ğŸ“‹ Má»¥c lá»¥c

- [Giá»›i thiá»‡u](#giá»›i-thiá»‡u)
- [YÃªu cáº§u há»‡ thá»‘ng](#yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
- [Cáº¥u trÃºc dá»± Ã¡n](#cáº¥u-trÃºc-dá»±-Ã¡n)
- [HÆ°á»›ng dáº«n sá»­ dá»¥ng](#hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
- [So sÃ¡nh hiá»‡u suáº¥t](#so-sÃ¡nh-hiá»‡u-suáº¥t)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Giá»›i thiá»‡u

Project nÃ y giÃºp báº¡n:
- Hiá»ƒu cÃ¡ch xuáº¥t mÃ´ hÃ¬nh PyTorch sang ONNX
- Cháº¡y inference vá»›i ONNX Runtime
- Tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh vá»›i TensorRT
- So sÃ¡nh hiá»‡u suáº¥t giá»¯a cÃ¡c framework

**CÃ¡c bÆ°á»›c thá»±c hiá»‡n:**
1. Export YOLOv8 â†’ ONNX
2. Inference vá»›i ONNX Runtime
3. Convert ONNX â†’ TensorRT Engine
4. Inference vá»›i TensorRT (GPU)
5. So sÃ¡nh performance

## ğŸ’» YÃªu cáº§u há»‡ thá»‘ng

### Cho ONNX Runtime (CPU/GPU):
- Python 3.8+
- Windows/Linux/MacOS
- (TÃ¹y chá»n) NVIDIA GPU vá»›i CUDA 11.x hoáº·c 12.x

### Cho TensorRT (chá»‰ GPU):
- NVIDIA GPU (Compute Capability â‰¥ 6.0)
- CUDA Toolkit 11.x hoáº·c 12.x
- cuDNN 8.x
- TensorRT 8.x hoáº·c 10.x
- Linux hoáº·c Windows

## ğŸ“¦ CÃ i Ä‘áº·t

### BÆ°á»›c 1: Clone hoáº·c táº¡o project

```bash
cd e:\AI\yolov8-onnx-tensorrt
```

### BÆ°á»›c 2: Táº¡o virtual environment (khuyáº¿n nghá»‹)

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### BÆ°á»›c 3: CÃ i Ä‘áº·t dependencies

#### CÃ i Ä‘áº·t cÆ¡ báº£n (CPU + ONNX Runtime):
```bash
pip install -r requirements.txt
```

#### CÃ i Ä‘áº·t cho GPU (ONNX Runtime GPU):
```bash
pip install onnxruntime-gpu  # Thay vÃ¬ onnxruntime
```

#### CÃ i Ä‘áº·t TensorRT (GPU only):

**Linux:**
```bash
pip install tensorrt
pip install pycuda
```

**Windows:**
1. Táº£i TensorRT tá»« [NVIDIA Developer](https://developer.nvidia.com/tensorrt)
2. Giáº£i nÃ©n vÃ  thÃªm vÃ o PATH
3. CÃ i Ä‘áº·t Python wheel:
   ```bash
   pip install tensorrt-10.x.x-cp3x-none-win_amd64.whl
   pip install pycuda
   ```

### BÆ°á»›c 4: Chuáº©n bá»‹ áº£nh test

Táº¡o thÆ° má»¥c `images` vÃ  Ä‘áº·t áº£nh test vÃ o Ä‘Ã³:

```bash
mkdir images
# Copy áº£nh cá»§a báº¡n vÃ o images/sample.jpg
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
yolov8-onnx-tensorrt/
â”œâ”€â”€ utils/                      # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ preprocessing.py        # Tiá»n xá»­ lÃ½ vÃ  háº­u xá»­ lÃ½
â”œâ”€â”€ images/                     # ThÆ° má»¥c chá»©a áº£nh test
â”‚   â””â”€â”€ sample.jpg
â”œâ”€â”€ models/                     # ThÆ° má»¥c lÆ°u models (tá»± Ä‘á»™ng táº¡o)
â”œâ”€â”€ 1_export_onnx.py           # Script 1: Export YOLOv8 â†’ ONNX
â”œâ”€â”€ 2_onnx_inference.py        # Script 2: Inference vá»›i ONNX Runtime
â”œâ”€â”€ 3_tensorrt_convert.py      # Script 3: Convert ONNX â†’ TensorRT
â”œâ”€â”€ 4_tensorrt_inference.py    # Script 4: Inference vá»›i TensorRT
â”œâ”€â”€ demo.py                    # Demo so sÃ¡nh ONNX vs TensorRT
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # File nÃ y
```

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### Script 1: Export ONNX

Xuáº¥t mÃ´ hÃ¬nh YOLOv8 sang Ä‘á»‹nh dáº¡ng ONNX:

```bash
python 1_export_onnx.py
```

**Output:** `yolov8n.onnx` (khoáº£ng 6MB)

**CÃ¡c model size khÃ¡c:**
- `yolov8n.pt` - Nano (nhanh nháº¥t, 6MB)
- `yolov8s.pt` - Small (9MB)
- `yolov8m.pt` - Medium (26MB)
- `yolov8l.pt` - Large (44MB)
- `yolov8x.pt` - Extra Large (68MB, chÃ­nh xÃ¡c nháº¥t)

### Script 2: ONNX Runtime Inference

Cháº¡y inference vá»›i ONNX Runtime:

```bash
python 2_onnx_inference.py
```

**Output:** 
- Káº¿t quáº£ detection trÃªn terminal
- áº¢nh vá»›i bounding boxes: `images/sample_onnx_result.jpg`
- Hiá»ƒn thá»‹ áº£nh káº¿t quáº£

**TÃ¹y chá»‰nh:**
```python
# Trong file 2_onnx_inference.py
onnx_path = "yolov8n.onnx"
image_path = "images/your_image.jpg"
use_gpu = True  # False náº¿u chá»‰ dÃ¹ng CPU
```

### Script 3: Convert sang TensorRT

Chuyá»ƒn Ä‘á»•i ONNX sang TensorRT engine (yÃªu cáº§u GPU):

```bash
python 3_tensorrt_convert.py
```

**Output:** `yolov8n_fp16.engine`

**CÃ¡c precision mode:**
- `fp32` - Äá»™ chÃ­nh xÃ¡c cao nháº¥t, cháº­m nháº¥t
- `fp16` - CÃ¢n báº±ng (khuyáº¿n nghá»‹) - nhanh hÆ¡n ~2x
- `int8` - Nhanh nháº¥t, cáº§n calibration

**TÃ¹y chá»‰nh:**
```python
# Trong file 3_tensorrt_convert.py
precision='fp16'  # Äá»•i thÃ nh 'fp32' hoáº·c 'int8'
max_workspace_size=2  # GB, tÄƒng náº¿u cÃ³ nhiá»u RAM
```

### Script 4: TensorRT Inference

Cháº¡y inference vá»›i TensorRT (yÃªu cáº§u GPU):

```bash
python 4_tensorrt_inference.py
```

**Output:**
- Káº¿t quáº£ detection trÃªn terminal
- áº¢nh vá»›i bounding boxes: `images/sample_tensorrt_result.jpg`
- Thá»i gian inference (ms) vÃ  FPS

### Demo: So sÃ¡nh ONNX vs TensorRT

Cháº¡y cáº£ hai vÃ  so sÃ¡nh hiá»‡u suáº¥t:

```bash
python demo.py
```

**Output:**
- So sÃ¡nh side-by-side ONNX vs TensorRT
- Metrics: inference time, FPS, speedup
- áº¢nh so sÃ¡nh: `comparison_result.jpg`

## ğŸ“Š So sÃ¡nh hiá»‡u suáº¥t

### Káº¿t quáº£ thá»±c táº¿ trÃªn NVIDIA RTX 4050

Benchmark vá»›i YOLOv8n, input size 640x640 (GPU vs GPU):

| Framework | Device | Precision | Inference Time | FPS | Speedup |
|-----------|--------|-----------|----------------|-----|---------|
| ONNX Runtime | GPU (RTX 4050) | FP32 | 7.90 ms | 126.57 | 1x (baseline) |
| TensorRT | GPU (RTX 4050) | FP16 | 2.27 ms | 439.72 | **3.47x** ğŸš€ |

### Chi tiáº¿t káº¿t quáº£:

**ONNX Runtime (GPU RTX 4050 vá»›i CUDA 12.x):**
- â±ï¸ Thá»i gian: 7.90 ms
- ğŸ“Š FPS: 126.57
- ğŸ¯ Detections: 2 objects
- âœ… Cháº¡y trÃªn GPU vá»›i CUDA ExecutionProvider

**TensorRT (GPU RTX 4050):**
- âš¡ Thá»i gian: 2.27 ms
- ğŸš€ FPS: 439.72 (gáº§n 440 FPS!)
- ğŸ¯ Detections: 2 objects
- ğŸ’š Tiáº¿t kiá»‡m thá»i gian: 71.2%

### So sÃ¡nh tá»•ng quan:

**TensorRT vs ONNX Runtime (GPU):** Nhanh hÆ¡n **3.47x**  
**TensorRT vs CPU:** Nhanh hÆ¡n **~16x** (440 FPS vs 27 FPS)  
**ONNX GPU vs CPU:** Nhanh hÆ¡n **~4.7x** (127 FPS vs 27 FPS)

**LÆ°u Ã½:** 
- Cáº§n cÃ i CUDA Toolkit 12.x Ä‘á»ƒ ONNX Runtime cháº¡y trÃªn GPU
- TensorRT tá»‘i Æ°u hÆ¡n cho inference trÃªn NVIDIA GPU
- Hiá»‡u suáº¥t phá»¥ thuá»™c vÃ o GPU, model size, input size

### Æ¯u Ä‘iá»ƒm tá»«ng framework:

**ONNX Runtime:**
- âœ… Cross-platform (CPU/GPU/Mobile)
- âœ… Dá»… sá»­ dá»¥ng
- âœ… KhÃ´ng cáº§n setup phá»©c táº¡p
- âŒ Cháº­m hÆ¡n TensorRT

**TensorRT:**
- âœ… Cá»±c ká»³ nhanh trÃªn GPU NVIDIA
- âœ… Tá»‘i Æ°u hÃ³a tá»± Ä‘á»™ng
- âœ… Há»— trá»£ FP16, INT8
- âŒ Chá»‰ cháº¡y trÃªn NVIDIA GPU
- âŒ Setup phá»©c táº¡p hÆ¡n

## ğŸ”§ Troubleshooting

### Lá»—i: "No module named 'ultralytics'"
```bash
pip install ultralytics
```

### Lá»—i: "No module named 'onnxruntime'"
```bash
pip install onnxruntime-gpu  # hoáº·c onnxruntime cho CPU
```

### Lá»—i: "No module named 'tensorrt'"
- Äáº£m báº£o Ä‘Ã£ cÃ i TensorRT Ä‘Ãºng cÃ¡ch
- Kiá»ƒm tra CUDA vÃ  cuDNN
- Xem hÆ°á»›ng dáº«n: https://docs.nvidia.com/deeplearning/tensorrt/install-guide/

### Lá»—i: "CUDA out of memory"
- Giáº£m `max_workspace_size` trong script 3
- Sá»­ dá»¥ng áº£nh nhá» hÆ¡n
- ÄÃ³ng cÃ¡c á»©ng dá»¥ng khÃ¡c Ä‘ang dÃ¹ng GPU

### ONNX Runtime cháº¡y cháº­m trÃªn GPU
- Kiá»ƒm tra provider: `print(ort.get_device())`
- CÃ i Ä‘áº·t `onnxruntime-gpu` thay vÃ¬ `onnxruntime`
- Kiá»ƒm tra CUDA: `nvidia-smi`

### TensorRT build engine lÃ¢u
- BÃ¬nh thÆ°á»ng, láº§n Ä‘áº§u build cÃ³ thá»ƒ máº¥t 2-5 phÃºt
- Engine Ä‘Æ°á»£c lÆ°u láº¡i, láº§n sau load nhanh
- TÄƒng `verbose=True` Ä‘á»ƒ xem tiáº¿n trÃ¬nh

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [YOLOv8 Documentation](https://docs.ultralytics.com/)
- [ONNX Runtime](https://onnxruntime.ai/)
- [TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)
- [ONNX GitHub](https://github.com/onnx/onnx)

## ğŸ“ License

MIT License - Tá»± do sá»­ dá»¥ng cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  thÆ°Æ¡ng máº¡i.

## ğŸ¤ ÄÃ³ng gÃ³p

Má»i Ä‘Ã³ng gÃ³p Ä‘á»u Ä‘Æ°á»£c hoan nghÃªnh! Táº¡o issue hoáº·c pull request náº¿u báº¡n cÃ³ cáº£i tiáº¿n.

---

**Happy Learning! ğŸš€**

Náº¿u gáº·p váº¥n Ä‘á», hÃ£y kiá»ƒm tra pháº§n [Troubleshooting](#troubleshooting) hoáº·c táº¡o issue.
